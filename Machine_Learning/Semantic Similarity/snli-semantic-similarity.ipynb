{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install transformers","execution_count":2,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (2.11.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers) (1.18.5)\nRequirement already satisfied: tokenizers==0.7.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.7.0)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from transformers) (0.1.91)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.45.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers) (20.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.10)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.43)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.23.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2020.4.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (1.14.0)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.1)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.24.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2020.6.20)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.9)\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.0.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport transformers","execution_count":3,"outputs":[{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# detect and init the TPU\n#tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n#tf.config.experimental_connect_to_cluster(tpu)\n#tf.tpu.experimental.initialize_tpu_system(tpu)\n# detect and init the TPU\n'''tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\nprint(\"Done\")\n\n# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Ten\n    sorflow. Works on CPU and single GPU.'''","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"'tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\\ntf.config.experimental_connect_to_cluster(tpu)\\ntf.tpu.experimental.initialize_tpu_system(tpu)\\n\\n# instantiate a distribution strategy\\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\\n\\n# instantiate a distribution strategy\\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\\nprint(\"Done\")\\n\\n# Detect hardware, return appropriate distribution strategy\\ntry:\\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\\n    print(\\'Running on TPU \\', tpu.master())\\nexcept ValueError:\\n    tpu = None\\n\\nif tpu:\\n    tf.config.experimental_connect_to_cluster(tpu)\\n    tf.tpu.experimental.initialize_tpu_system(tpu)\\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\\nelse:\\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Ten\\n    sorflow. Works on CPU and single GPU.'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Configuration\nmax_length = 128 #Max length of input sentence\nbatch_size = 32\nepochs = 2","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Load the data\n!curl -LO https://raw.githubusercontent.com/MohamadMerchant/SNLI/master/data.tar.gz\n!tar -xvzf data.tar.gz","execution_count":6,"outputs":[{"output_type":"stream","text":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 11.1M  100 11.1M    0     0  5553k      0  0:00:02  0:00:02 --:--:-- 5553k\nSNLI_Corpus/\nSNLI_Corpus/snli_1.0_dev.csv\nSNLI_Corpus/snli_1.0_train.csv\nSNLI_Corpus/snli_1.0_test.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# There are more than 550k samples in total; we will use 100k for this example.\ntrain_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_train.csv\", nrows=100000)\nvalid_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_dev.csv\")\ntest_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_test.csv\")\n\n# Shape of the data\nprint(f\"Total train samples : {train_df.shape[0]}\")\nprint(f\"Total validation samples: {valid_df.shape[0]}\")\nprint(f\"Total test samples: {valid_df.shape[0]}\")","execution_count":7,"outputs":[{"output_type":"stream","text":"Total train samples : 100000\nTotal validation samples: 10000\nTotal test samples: 10000\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"      similarity                                          sentence1  \\\n0        neutral  A person on a horse jumps over a broken down a...   \n1  contradiction  A person on a horse jumps over a broken down a...   \n2     entailment  A person on a horse jumps over a broken down a...   \n3        neutral              Children smiling and waving at camera   \n4     entailment              Children smiling and waving at camera   \n\n                                           sentence2  \n0  A person is training his horse for a competition.  \n1      A person is at a diner, ordering an omelette.  \n2                  A person is outdoors, on a horse.  \n3                  They are smiling at their parents  \n4                         There are children present  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>similarity</th>\n      <th>sentence1</th>\n      <th>sentence2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>neutral</td>\n      <td>A person on a horse jumps over a broken down a...</td>\n      <td>A person is training his horse for a competition.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>contradiction</td>\n      <td>A person on a horse jumps over a broken down a...</td>\n      <td>A person is at a diner, ordering an omelette.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>entailment</td>\n      <td>A person on a horse jumps over a broken down a...</td>\n      <td>A person is outdoors, on a horse.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>neutral</td>\n      <td>Children smiling and waving at camera</td>\n      <td>They are smiling at their parents</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>entailment</td>\n      <td>Children smiling and waving at camera</td>\n      <td>There are children present</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_df.head()","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"      similarity                                          sentence1  \\\n0        neutral  Two women are embracing while holding to go pa...   \n1     entailment  Two women are embracing while holding to go pa...   \n2  contradiction  Two women are embracing while holding to go pa...   \n3     entailment  Two young children in blue jerseys, one with t...   \n4        neutral  Two young children in blue jerseys, one with t...   \n\n                                           sentence2  \n0  The sisters are hugging goodbye while holding ...  \n1                    Two woman are holding packages.  \n2               The men are fighting outside a deli.  \n3     Two kids in numbered jerseys wash their hands.  \n4           Two kids at a ballgame wash their hands.  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>similarity</th>\n      <th>sentence1</th>\n      <th>sentence2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>neutral</td>\n      <td>Two women are embracing while holding to go pa...</td>\n      <td>The sisters are hugging goodbye while holding ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>entailment</td>\n      <td>Two women are embracing while holding to go pa...</td>\n      <td>Two woman are holding packages.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>contradiction</td>\n      <td>Two women are embracing while holding to go pa...</td>\n      <td>The men are fighting outside a deli.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>entailment</td>\n      <td>Two young children in blue jerseys, one with t...</td>\n      <td>Two kids in numbered jerseys wash their hands.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>neutral</td>\n      <td>Two young children in blue jerseys, one with t...</td>\n      <td>Two kids at a ballgame wash their hands.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":10,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 100000 entries, 0 to 99999\nData columns (total 3 columns):\n #   Column      Non-Null Count   Dtype \n---  ------      --------------   ----- \n 0   similarity  100000 non-null  object\n 1   sentence1   100000 non-null  object\n 2   sentence2   99997 non-null   object\ndtypes: object(3)\nmemory usage: 2.3+ MB\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Labels in our dataset.\nlabels = [\"contradiction\", \"entailment\", \"neutral\"]","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Sentence1 : {train_df.loc[1, 'sentence1']}\")\nprint(f\"Sentence2: {train_df.loc[1, 'sentence2']}\")\nprint(f\"Similarity: {train_df.loc[1, 'similarity']}\")","execution_count":12,"outputs":[{"output_type":"stream","text":"Sentence1 : A person on a horse jumps over a broken down airplane.\nSentence2: A person is at a diner, ordering an omelette.\nSimilarity: contradiction\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Preprocessing\nprint(\"Number of missing values\")\nprint(train_df.isnull().sum())","execution_count":13,"outputs":[{"output_type":"stream","text":"Number of missing values\nsimilarity    0\nsentence1     0\nsentence2     3\ndtype: int64\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We Drop the missing values\ntrain_df.dropna(axis =0, inplace = True)\nprint(train_df.isnull().sum())","execution_count":14,"outputs":[{"output_type":"stream","text":"similarity    0\nsentence1     0\nsentence2     0\ndtype: int64\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Observe the distribution of training targets\nprint(\"Training data target distribution\")\nprint(train_df.similarity.value_counts())","execution_count":15,"outputs":[{"output_type":"stream","text":"Training data target distribution\nentailment       33384\ncontradiction    33310\nneutral          33193\n-                  110\nName: similarity, dtype: int64\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Observe the validation data set distribution\nprint(\"Validation datatarget distribution\")\nprint(valid_df.similarity.value_counts())","execution_count":16,"outputs":[{"output_type":"stream","text":"Validation datatarget distribution\nentailment       3329\ncontradiction    3278\nneutral          3235\n-                 158\nName: similarity, dtype: int64\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = (\n    train_df[train_df.similarity != \"-\"]\n    .sample(frac = 1.0, random_state = 42)\n    .reset_index(drop = True)\n)","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_df = (\n    valid_df[valid_df.similarity != \"-\"]\n    .sample(frac= 1.0, random_state = 42)\n    .reset_index(drop = True)\n)","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.similarity.value_counts())","execution_count":19,"outputs":[{"output_type":"stream","text":"entailment       33384\ncontradiction    33310\nneutral          33193\nName: similarity, dtype: int64\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(valid_df.similarity.value_counts())","execution_count":20,"outputs":[{"output_type":"stream","text":"entailment       3329\ncontradiction    3278\nneutral          3235\nName: similarity, dtype: int64\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#One hot encoding of similarity column in training, testing and validation data\ntrain_df[\"label\"] = train_df[\"similarity\"].apply(\n    lambda x: 0 if x == \"contradiction\" else 1 if x == \"entailment\" else 2\n)\ny_train = tf.keras.utils.to_categorical(train_df.label, num_classes=3)\n\nvalid_df[\"label\"] = valid_df[\"similarity\"].apply(\n    lambda x: 0 if x == \"contradiction\" else 1 if x == \"entailment\" else 2\n)\ny_val = tf.keras.utils.to_categorical(valid_df.label, num_classes=3)\n\ntest_df[\"label\"] = test_df[\"similarity\"].apply(\n    lambda x: 0 if x == \"contradiction\" else 1 if x == \"entailment\" else 2\n)\ny_test = tf.keras.utils.to_categorical(test_df.label, num_classes=3)","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create Custom Data generator\n\n\"\"\"Generates batches of data.\n\n    Args:\n        sentence_pairs: Array of premise and hypothesis input sentences.\n        labels: Array of labels.\n        batch_size: Integer batch size.\n        shuffle: boolean, whether to shuffle the data.\n        include_targets: boolean, whether to incude the labels.\n\n    Returns:\n        Tuples `([input_ids, attention_mask, `token_type_ids], labels)`\n        (or just `[input_ids, attention_mask, `token_type_ids]`\n         if `include_targets=False`)\n\"\"\"\nclass BertSemanticDataGenerator(tf.keras.utils.Sequence):\n    def __init__(self,sentence_pairs,labels,batch_size=batch_size,shuffle=True,include_targets=True) :\n        self.sentence_pairs = sentence_pairs\n        self.labels = labels\n        self.shuffle = shuffle\n        self.batch_size = batch_size\n        self.include_targets = include_targets\n        # Load our BERT Tokenizer to encode the text.\n        # We will use base-base-uncased pretrained model.\n        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n            \"bert-base-uncased\", do_lower_case=True\n        )\n        self.indexes = np.arange(len(self.sentence_pairs))\n        self.on_epoch_end()\n\n    def __len__(self):\n        return len(self.sentence_pairs)// self.batch_size\n        #Returns the number of batches per epoch\n\n    def __getitem__(self, idx):\n        #Retrieves the batch of index\n        indexes = self.indexes[idx * self.batch_size : (idx+1) * self.batch_size ]\n        sentence_pairs = self.sentence_pairs[indexes]\n\n        #BERT batch_encode_plus batch the sentences and encode together\n        # and seperated by [SEP] token\n\n        encoded = self.tokenizer.batch_encode_plus(\n            sentence_pairs.tolist(),\n            add_special_tokens = True,\n            max_length = max_length,\n            return_attention_mask = True,\n            return_token_type_ids = True,\n            pad_to_max_length = True,\n            return_tensors = \"tf\",\n        ) \n\n        input_ids = np.array(encoded[\"input_ids\"], dtype = \"int32\")\n        attention_masks = np.array(encoded[\"attention_mask\"], dtype = \"int32\")\n        token_type_ids =  np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n\n        # Set to true if data generator is used for training/validation.\n        if self.include_targets:\n            labels = np.array(self.labels[indexes], dtype=\"int32\")\n            return [input_ids, attention_masks, token_type_ids], labels\n        else:\n            return [input_ids, attention_masks, token_type_ids]\n\n    def on_epoch_end(self):\n        # Shuffle indexes after each epoch if shuffle is set to True.\n        if self.shuffle:\n            np.random.RandomState(42).shuffle(self.indexes)\n\n","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the model under a distribution strategy scope.\nstrategy = tf.distribute.MirroredStrategy()\n# detect and init the TPU\n#tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n#tf.config.experimental_connect_to_cluster(tpu)\n#tf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\n#tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n# instantiating the model in the strategy scope creates the model on the TPU\nwith strategy.scope():\n\n#with strategy.scope():\n    # Encoded token ids from BERT tokenizer.\n    input_ids = tf.keras.layers.Input(\n        shape=(max_length,), dtype=tf.int32, name=\"input_ids\"\n    )\n    # Attention masks indicates to the model which tokens should be attended to.\n    attention_masks = tf.keras.layers.Input(\n        shape=(max_length,), dtype=tf.int32, name=\"attention_masks\"\n    )\n    # Token type ids are binary masks identifying different sequences in the model.\n    token_type_ids = tf.keras.layers.Input(\n        shape=(max_length,), dtype=tf.int32, name=\"token_type_ids\"\n    )\n    # Loading pretrained BERT model.\n    bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\n    # Freeze the BERT model to reuse the pretrained features without modifying them.\n    bert_model.trainable = False\n\n    sequence_output, pooled_output = bert_model(\n        input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n    )\n    # Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.\n    bi_lstm = tf.keras.layers.Bidirectional(\n        tf.keras.layers.LSTM(64, return_sequences=True)\n    )(sequence_output) # Applying hybrid pooling approach to bi_lstm sequence output.\n    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n    max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n    concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n    dropout = tf.keras.layers.Dropout(0.3)(concat)\n    output = tf.keras.layers.Dense(3, activation=\"softmax\")(dropout)\n    model = tf.keras.models.Model(\n        inputs=[input_ids, attention_masks, token_type_ids], outputs=output\n    )\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(),\n        loss=\"categorical_crossentropy\",\n        metrics=[\"acc\"],\n    )\n\n\nprint(f\"Strategy: {strategy}\")\nmodel.summary()","execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a09bd15e25584bc4a6b58997bb8b0a43"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=536063208.0, style=ProgressStyle(descri…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff2be6599d264bc58068e544f56947aa"}},"metadata":{}},{"output_type":"stream","text":"\nStrategy: <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7f10d97f8fd0>\nModel: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_ids (InputLayer)          [(None, 128)]        0                                            \n__________________________________________________________________________________________________\nattention_masks (InputLayer)    [(None, 128)]        0                                            \n__________________________________________________________________________________________________\ntoken_type_ids (InputLayer)     [(None, 128)]        0                                            \n__________________________________________________________________________________________________\ntf_bert_model (TFBertModel)     ((None, 128, 768), ( 109482240   input_ids[0][0]                  \n__________________________________________________________________________________________________\nbidirectional (Bidirectional)   (None, 128, 128)     426496      tf_bert_model[0][0]              \n__________________________________________________________________________________________________\nglobal_average_pooling1d (Globa (None, 128)          0           bidirectional[0][0]              \n__________________________________________________________________________________________________\nglobal_max_pooling1d (GlobalMax (None, 128)          0           bidirectional[0][0]              \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 256)          0           global_average_pooling1d[0][0]   \n                                                                 global_max_pooling1d[0][0]       \n__________________________________________________________________________________________________\ndropout_37 (Dropout)            (None, 256)          0           concatenate[0][0]                \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 3)            771         dropout_37[0][0]                 \n==================================================================================================\nTotal params: 109,909,507\nTrainable params: 427,267\nNon-trainable params: 109,482,240\n__________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = BertSemanticDataGenerator(\n    train_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\"),\n    y_train,\n    batch_size=batch_size,\n    shuffle=True,\n)\nvalid_data = BertSemanticDataGenerator(\n    valid_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\"),\n    y_val,\n    batch_size=batch_size,\n    shuffle=False,\n)","execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"771ef398873f4d18ae2e6b776cfbd75c"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(\n    train_data,\n    validation_data=valid_data,\n    epochs=epochs,\n    use_multiprocessing=True,\n    workers=-1,\n)","execution_count":25,"outputs":[{"output_type":"stream","text":"Epoch 1/2\n3121/3121 [==============================] - 661s 212ms/step - loss: 0.6972 - acc: 0.7028 - val_loss: 0.5447 - val_acc: 0.7830\nEpoch 2/2\n3121/3121 [==============================] - 656s 210ms/step - loss: 0.5961 - acc: 0.7579 - val_loss: 0.5072 - val_acc: 0.7971\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Unfreeze the bert_model.\nbert_model.trainable = True\n# Recompile the model to make the change effective.\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(1e-5),\n    loss=\"categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n)\nmodel.summary()","execution_count":26,"outputs":[{"output_type":"stream","text":"Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_ids (InputLayer)          [(None, 128)]        0                                            \n__________________________________________________________________________________________________\nattention_masks (InputLayer)    [(None, 128)]        0                                            \n__________________________________________________________________________________________________\ntoken_type_ids (InputLayer)     [(None, 128)]        0                                            \n__________________________________________________________________________________________________\ntf_bert_model (TFBertModel)     ((None, 128, 768), ( 109482240   input_ids[0][0]                  \n__________________________________________________________________________________________________\nbidirectional (Bidirectional)   (None, 128, 128)     426496      tf_bert_model[0][0]              \n__________________________________________________________________________________________________\nglobal_average_pooling1d (Globa (None, 128)          0           bidirectional[0][0]              \n__________________________________________________________________________________________________\nglobal_max_pooling1d (GlobalMax (None, 128)          0           bidirectional[0][0]              \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 256)          0           global_average_pooling1d[0][0]   \n                                                                 global_max_pooling1d[0][0]       \n__________________________________________________________________________________________________\ndropout_37 (Dropout)            (None, 256)          0           concatenate[0][0]                \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 3)            771         dropout_37[0][0]                 \n==================================================================================================\nTotal params: 109,909,507\nTrainable params: 109,909,507\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(\n    train_data,\n    validation_data=valid_data,\n    epochs=epochs,\n    use_multiprocessing=True,\n    workers=-1,\n)","execution_count":27,"outputs":[{"output_type":"stream","text":"Epoch 1/2\n3121/3121 [==============================] - 1571s 503ms/step - accuracy: 0.8174 - loss: 0.4697 - val_accuracy: 0.8650 - val_loss: 0.3580\nEpoch 2/2\n3121/3121 [==============================] - 1567s 502ms/step - accuracy: 0.8695 - loss: 0.3538 - val_accuracy: 0.8789 - val_loss: 0.3390\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = BertSemanticDataGenerator(\n    test_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\"),\n    y_test,\n    batch_size=batch_size,\n    shuffle=False,\n)\nmodel.evaluate(test_data, verbose=1)","execution_count":28,"outputs":[{"output_type":"stream","text":"312/312 [==============================] - 54s 174ms/step - accuracy: 0.8682 - loss: 0.3618\n","name":"stdout"},{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"[0.36180979013442993, 0.8681890964508057]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_similarity(sentence1, sentence2):\n    sentence_pairs = np.array([[str(sentence1), str(sentence2)]])\n    test_data = BertSemanticDataGenerator(\n        sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,\n    )\n\n    proba = model.predict(test_data)[0]\n    idx = np.argmax(proba)\n    proba = f\"{proba[idx]: .2f}%\"\n    pred = labels[idx]\n    return pred, proba","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence1 = \"Two women are observing something together.\"\nsentence2 = \"Two women are standing with their eyes closed.\"\ncheck_similarity(sentence1, sentence2)","execution_count":30,"outputs":[{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"('contradiction', ' 0.98%')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence1 = \"A soccer game with multiple males playing\"\nsentence2 = \"Some men are playing a sport\"\ncheck_similarity(sentence1, sentence2)","execution_count":31,"outputs":[{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"('entailment', ' 0.94%')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence1 = \"A smiling costumed woman is holding an umbrella\"\nsentence2 = \"A happy woman in a fairy costume holds an umbrella\"\ncheck_similarity(sentence1, sentence2)","execution_count":32,"outputs":[{"output_type":"execute_result","execution_count":32,"data":{"text/plain":"('neutral', ' 0.97%')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence1 = \"My name is Adam\"\nsentence2 = \"Adam is my name\"\nsentence3 = \"I am Adam\"\nsentence4 = \"My brother name is Adam\"\nprint(\"1 & 2 comparision : \",check_similarity(sentence1, sentence2))\nprint(\"1 & 3 comparision : \",check_similarity(sentence1, sentence3))\nprint(\"1 & 4 comparision : \",check_similarity(sentence1, sentence4))","execution_count":35,"outputs":[{"output_type":"stream","text":"1 & 2 comparision :  ('entailment', ' 0.96%')\n1 & 3 comparision :  ('entailment', ' 0.94%')\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}